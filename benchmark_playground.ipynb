{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Graph-Aware Remasking vs WINO Baseline - GSM8K Benchmark\n",
                "\n",
                "이 노트북은 **Graph-Aware Historical Remasking** 디코더와 **WINO** baseline을 GSM8K 벤치마크에서 비교합니다.\n",
                "\n",
                "## 목적\n",
                "- WINO (baseline): Confidence 기반 remasking\n",
                "- Graph-Remask (experimental): Attention 기반 responsibility + confidence remasking\n",
                "- 평가 지표: Accuracy, Forward passes, Remask count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Add current directory to path\n",
                "current_dir = os.getcwd()\n",
                "if current_dir not in sys.path:\n",
                "    sys.path.append(current_dir)\n",
                "\n",
                "# Import local modules\n",
                "from modeling_llada import LLaDAModelLM\n",
                "from configuration_llada import LLaDAConfig\n",
                "from decoding import decoding_wino, decoding_graph_remask\n",
                "import benchmark_utils\n",
                "\n",
                "print(\"Modules loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LOCAL_MODEL_PATH = \"../Grok-1-LLaDA-8B\"\n",
                "HF_MODEL_ID = \"GSAI-ML/LLaDA-8B-Base\"\n",
                "\n",
                "model_path = HF_MODEL_ID\n",
                "if os.path.exists(LOCAL_MODEL_PATH):\n",
                "    model_path = LOCAL_MODEL_PATH\n",
                "    print(f\"Using local model: {model_path}\")\n",
                "else:\n",
                "    print(f\"Using HuggingFace model: {model_path}\")\n",
                "\n",
                "config = LLaDAConfig.from_pretrained(model_path)\n",
                "model = LLaDAModelLM.from_pretrained(model_path, config=config, torch_dtype=\"auto\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    model.cuda()\n",
                "model.eval()\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "print(\"Model loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load GSM8K Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "N_SAMPLES = 50  # Number of GSM8K samples to test\n",
                "GEN_LENGTH = 256  # As requested\n",
                "BLOCK_LENGTH = 256\n",
                "\n",
                "print(f\"Loading GSM8K dataset (N={N_SAMPLES})...\")\n",
                "gsm8k_data = benchmark_utils.load_gsm8k(n_samples=N_SAMPLES)\n",
                "print(f\"Loaded {len(gsm8k_data)} samples.\")\n",
                "\n",
                "# Show example\n",
                "if gsm8k_data:\n",
                "    print(\"\\nExample question:\")\n",
                "    print(gsm8k_data[0]['question'])\n",
                "    print(f\"\\nGround truth: {gsm8k_data[0]['ground_truth']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run Benchmark Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "mask_id = 126336\n",
                "\n",
                "for idx, item in enumerate(gsm8k_data):\n",
                "    question = item['question']\n",
                "    ground_truth = item['ground_truth']\n",
                "    \n",
                "    # Encode prompt\n",
                "    prompt_tokens = tokenizer.encode(question, return_tensors='pt').to(model.device)\n",
                "    \n",
                "    print(f\"\\n[{idx+1}/{len(gsm8k_data)}] Processing...\")\n",
                "    \n",
                "    # === WINO Baseline ===\n",
                "    try:\n",
                "        output_wino, steps_wino = decoding_wino(\n",
                "            model=model,\n",
                "            prompt=prompt_tokens,\n",
                "            gen_length=GEN_LENGTH,\n",
                "            block_length=BLOCK_LENGTH,\n",
                "            temperature=0.0,\n",
                "            mask_id=mask_id,\n",
                "            threshold=0.6,\n",
                "            threshold_back=0.9\n",
                "        )\n",
                "        text_wino = tokenizer.decode(output_wino[0], skip_special_tokens=True)\n",
                "        correct_wino = benchmark_utils.check_correctness(text_wino, ground_truth, \"Math\")\n",
                "        \n",
                "        print(f\"  WINO: {steps_wino} steps, Correct: {correct_wino}\")\n",
                "    except Exception as e:\n",
                "        print(f\"  WINO failed: {e}\")\n",
                "        steps_wino = -1\n",
                "        correct_wino = False\n",
                "        text_wino = \"\"\n",
                "    \n",
                "    # === Graph-Aware Remasking ===\n",
                "    try:\n",
                "        output_graph, stats_graph = decoding_graph_remask(\n",
                "            model=model,\n",
                "            prompt=prompt_tokens,\n",
                "            gen_length=GEN_LENGTH,\n",
                "            block_length=BLOCK_LENGTH,\n",
                "            temperature=0.0,\n",
                "            mask_id=mask_id,\n",
                "            threshold_forward=0.6,\n",
                "            threshold_back=0.9,\n",
                "            resp_threshold=0.3,\n",
                "            gamma_decay=0.95,\n",
                "            use_attention_layers=[-1],\n",
                "            top_k_attention=10,\n",
                "            max_remask_ratio=0.3\n",
                "        )\n",
                "        text_graph = tokenizer.decode(output_graph[0], skip_special_tokens=True)\n",
                "        correct_graph = benchmark_utils.check_correctness(text_graph, ground_truth, \"Math\")\n",
                "        steps_graph = stats_graph['total_steps']\n",
                "        remasks_graph = stats_graph['total_remasks']\n",
                "        \n",
                "        print(f\"  Graph: {steps_graph} steps, {remasks_graph} remasks, Correct: {correct_graph}\")\n",
                "    except Exception as e:\n",
                "        print(f\"  Graph-Remask failed: {e}\")\n",
                "        steps_graph = -1\n",
                "        remasks_graph = -1\n",
                "        correct_graph = False\n",
                "        text_graph = \"\"\n",
                "    \n",
                "    # Store results\n",
                "    results.append({\n",
                "        'question': question,\n",
                "        'ground_truth': ground_truth,\n",
                "        'wino_correct': 1 if correct_wino else 0,\n",
                "        'wino_steps': steps_wino,\n",
                "        'graph_correct': 1 if correct_graph else 0,\n",
                "        'graph_steps': steps_graph,\n",
                "        'graph_remasks': remasks_graph,\n",
                "        'wino_output': text_wino,\n",
                "        'graph_output': text_graph\n",
                "    })\n",
                "\n",
                "# Create DataFrame\n",
                "df_results = pd.DataFrame(results)\n",
                "print(\"\\n=== Benchmark Complete ===\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary statistics\n",
                "print(\"\\n=== Summary Statistics ===\")\n",
                "print(f\"\\nWINO Baseline:\")\n",
                "print(f\"  Accuracy: {df_results['wino_correct'].mean():.2%}\")\n",
                "print(f\"  Avg Steps: {df_results['wino_steps'].mean():.1f}\")\n",
                "\n",
                "print(f\"\\nGraph-Aware Remasking:\")\n",
                "print(f\"  Accuracy: {df_results['graph_correct'].mean():.2%}\")\n",
                "print(f\"  Avg Steps: {df_results['graph_steps'].mean():.1f}\")\n",
                "print(f\"  Avg Remasks: {df_results['graph_remasks'].mean():.1f}\")\n",
                "\n",
                "print(f\"\\nImprovement:\")\n",
                "acc_diff = df_results['graph_correct'].mean() - df_results['wino_correct'].mean()\n",
                "print(f\"  Accuracy Δ: {acc_diff:+.2%}\")\n",
                "step_diff = df_results['graph_steps'].mean() - df_results['wino_steps'].mean()\n",
                "print(f\"  Steps Δ: {step_diff:+.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "# Accuracy comparison\n",
                "ax1 = axes[0]\n",
                "accuracies = [df_results['wino_correct'].mean(), df_results['graph_correct'].mean()]\n",
                "ax1.bar(['WINO', 'Graph-Remask'], accuracies, color=['#3498db', '#e74c3c'])\n",
                "ax1.set_ylabel('Accuracy')\n",
                "ax1.set_title('GSM8K Accuracy Comparison')\n",
                "ax1.set_ylim([0, 1])\n",
                "for i, v in enumerate(accuracies):\n",
                "    ax1.text(i, v + 0.02, f\"{v:.2%}\", ha='center', fontweight='bold')\n",
                "\n",
                "# Steps comparison\n",
                "ax2 = axes[1]\n",
                "steps = [df_results['wino_steps'].mean(), df_results['graph_steps'].mean()]\n",
                "ax2.bar(['WINO', 'Graph-Remask'], steps, color=['#3498db', '#e74c3c'])\n",
                "ax2.set_ylabel('Average Steps')\n",
                "ax2.set_title('Decoding Steps Comparison')\n",
                "for i, v in enumerate(steps):\n",
                "    ax2.text(i, v + 1, f\"{v:.1f}\", ha='center', fontweight='bold')\n",
                "\n",
                "# Remask count\n",
                "ax3 = axes[2]\n",
                "ax3.hist(df_results['graph_remasks'], bins=20, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
                "ax3.set_xlabel('Remask Count')\n",
                "ax3.set_ylabel('Frequency')\n",
                "ax3.set_title('Graph-Remask: Remask Distribution')\n",
                "ax3.axvline(df_results['graph_remasks'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_results['graph_remasks'].mean():.1f}\")\n",
                "ax3.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Failure Case Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find cases where WINO failed but Graph succeeded\n",
                "graph_wins = df_results[(df_results['wino_correct'] == 0) & (df_results['graph_correct'] == 1)]\n",
                "print(f\"\\n=== Graph-Remask Wins (WINO failed, Graph succeeded): {len(graph_wins)} cases ===\")\n",
                "if len(graph_wins) > 0:\n",
                "    for idx, row in graph_wins.head(3).iterrows():\n",
                "        print(f\"\\n--- Case {idx+1} ---\")\n",
                "        print(f\"Question: {row['question'][:100]}...\")\n",
                "        print(f\"Ground Truth: {row['ground_truth']}\")\n",
                "        print(f\"WINO Output: {benchmark_utils.extract_number(row['wino_output'])}\")\n",
                "        print(f\"Graph Output: {benchmark_utils.extract_number(row['graph_output'])}\")\n",
                "\n",
                "# Find cases where WINO succeeded but Graph failed\n",
                "wino_wins = df_results[(df_results['wino_correct'] == 1) & (df_results['graph_correct'] == 0)]\n",
                "print(f\"\\n=== WINO Wins (Graph failed, WINO succeeded): {len(wino_wins)} cases ===\")\n",
                "if len(wino_wins) > 0:\n",
                "    for idx, row in wino_wins.head(3).iterrows():\n",
                "        print(f\"\\n--- Case {idx+1} ---\")\n",
                "        print(f\"Question: {row['question'][:100]}...\")\n",
                "        print(f\"Ground Truth: {row['ground_truth']}\")\n",
                "        print(f\"WINO Output: {benchmark_utils.extract_number(row['wino_output'])}\")\n",
                "        print(f\"Graph Output: {benchmark_utils.extract_number(row['graph_output'])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to CSV\n",
                "output_file = \"graph_remask_vs_wino_gsm8k_results.csv\"\n",
                "df_results.to_csv(output_file, index=False)\n",
                "print(f\"\\nResults saved to {output_file}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}