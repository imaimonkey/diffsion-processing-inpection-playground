{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLaDA Benchmark Playground\n",
                "\n",
                "이 노트북은 **정량적 평가**를 위한 벤치마크 실험을 수행합니다.\n",
                "\n",
                "## 목적\n",
                "- Academic Benchmarks (GSM8K, MMLU)를 사용한 A/B 테스트\n",
                "- Baseline vs Experimental 샘플링 전략 비교\n",
                "- 메트릭: Accuracy, Perplexity, Stability, Survival Rate, Correction Efficacy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import pandas as pd\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "# Add current directory to path\n",
                "current_dir = os.getcwd()\n",
                "if current_dir not in sys.path:\n",
                "    sys.path.append(current_dir)\n",
                "\n",
                "# Import local modules\n",
                "from modeling_llada import LLaDAModelLM\n",
                "from configuration_llada import LLaDAConfig\n",
                "import experiment_utils\n",
                "import decoding\n",
                "from tta_uncertainty_sampling import generate_with_tta_uncertainty\n",
                "\n",
                "print(\"Modules loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LOCAL_MODEL_PATH = \"../Grok-1-LLaDA-8B\"\n",
                "HF_MODEL_ID = \"GSAI-ML/LLaDA-8B-Base\"\n",
                "\n",
                "model_path = HF_MODEL_ID\n",
                "if os.path.exists(LOCAL_MODEL_PATH):\n",
                "    model_path = LOCAL_MODEL_PATH\n",
                "    print(f\"Using local model: {model_path}\")\n",
                "else:\n",
                "    print(f\"Using HuggingFace model: {model_path}\")\n",
                "\n",
                "config = LLaDAConfig.from_pretrained(model_path)\n",
                "model = LLaDAModelLM.from_pretrained(model_path, config=config, torch_dtype=\"auto\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    model.cuda()\n",
                "model.eval()\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "print(\"Model loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Run Academic Benchmark\n",
                "\n",
                "이 셀은 GSM8K와 MMLU 데이터셋을 사용하여 **TTA Uncertainty Sampling**을 벤치마크합니다.\n",
                "\n",
                "### 설정 옵션:\n",
                "- **Temporal Decay 테스트**: `experimental_fn=None` (기본값)\n",
                "- **TTA 테스트**: `experimental_fn=generate_with_tta_uncertainty` (현재 설정)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark Configuration\n",
                "TTA_K_VALUES = [3, 5, 7]  # TTA forward pass 횟수\n",
                "N_SAMPLES = 20  # Number of samples per task (빠른 테스트용, 전체는 50)\n",
                "STEPS = 32  # 빠른 테스트용 (전체는 64)\n",
                "GEN_LENGTH = 32\n",
                "BLOCK_LENGTH = 32\n",
                "\n",
                "print(f\"Starting TTA Uncertainty Sampling Benchmark\")\n",
                "print(f\"TTA K Values: {TTA_K_VALUES}\")\n",
                "print(f\"Samples per task: {N_SAMPLES}\")\n",
                "print(f\"Expected time: ~{N_SAMPLES * len(TTA_K_VALUES) * 2} minutes\")\n",
                "\n",
                "results_df = experiment_utils.run_academic_benchmark(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    baseline_fn=None,  # Uses inspect_sampling (no remasking)\n",
                "    experimental_fn=generate_with_tta_uncertainty,  # TTA algorithm\n",
                "    thresholds=TTA_K_VALUES,  # Different tta_k values to test\n",
                "    samples=N_SAMPLES,\n",
                "    steps=STEPS,\n",
                "    gen_length=GEN_LENGTH,\n",
                "    block_length=BLOCK_LENGTH\n",
                ")\n",
                "\n",
                "print(\"\\nBenchmark completed!\")\n",
                "print(f\"Total results: {len(results_df)} rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display comprehensive analysis\n",
                "experiment_utils.analyze_icml_results(results_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save to CSV for further analysis\n",
                "output_file = \"tta_benchmark_results.csv\"\n",
                "results_df.to_csv(output_file, index=False)\n",
                "print(f\"Results saved to {output_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Detailed Inspection (Optional)\n",
                "\n",
                "특정 케이스를 자세히 살펴보고 싶다면 아래 셀을 사용하세요."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter for specific category or AlphaDecay (TTA K)\n",
                "print(\"\\n=== Math Task Results ===\")\n",
                "math_results = results_df[results_df['Category'] == 'math']\n",
                "print(math_results.groupby('AlphaDecay')[['Acc_Exp', 'PPL_Delta', 'Stability_Delta']].mean())\n",
                "\n",
                "print(\"\\n=== Logic Task Results ===\")\n",
                "logic_results = results_df[results_df['Category'] == 'logic']\n",
                "print(logic_results.groupby('AlphaDecay')[['Acc_Exp', 'PPL_Delta', 'Stability_Delta']].mean())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. TTA-Specific Analysis\n",
                "\n",
                "TTA 알고리즘의 불확실성 메트릭을 분석합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary by TTA K\n",
                "print(\"\\n=== Performance by TTA K ===\")\n",
                "summary = results_df.groupby('AlphaDecay')[['Acc_Exp', 'PPL_Delta', 'Stability_Delta', 'Survival', 'Correction_Eff']].mean()\n",
                "print(summary)\n",
                "\n",
                "# Best configuration\n",
                "best_k = summary['Acc_Exp'].idxmax()\n",
                "best_acc = summary['Acc_Exp'].max()\n",
                "baseline_acc = results_df['Acc_Base'].mean()\n",
                "\n",
                "print(f\"\\n=== Best Configuration ===\")\n",
                "print(f\"Best TTA K: {best_k}\")\n",
                "print(f\"Accuracy: {best_acc:.2%} (Baseline: {baseline_acc:.2%})\")\n",
                "print(f\"Improvement: {best_acc - baseline_acc:+.2%}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}