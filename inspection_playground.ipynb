{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaDA Diffusion Process Inspection & Experimentation\n",
    "\n",
    "이 노트북은 LLaDA 모델의 디퓨전 기반 텍스트 생성 과정을 단계별로 분석하고, **새로운 샘플링 알고리즘을 실험하여 베이스라인과 비교**하기 위해 작성되었습니다.\n",
    "\n",
    "## 주요 기능\n",
    "1. **Inspection**: 디퓨전 과정의 투명한 시각화 (Step-by-step Logits, Confidence).\n",
    "2. **Experimentation**: 커스텀 샘플링 알고리즘 구현 및 적용.\n",
    "3. **Comparison**: Baseline vs Custom 알고리즘의 성능(NFE, 속도, 품질) 정량 비교."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== BASIC SYSTEM INFO =====\n",
      "OS            : Linux 5.15.0-78-generic\n",
      "Python        : 3.12.12\n",
      "CPU           : x86_64\n",
      "CPU Cores     : 64\n",
      "\n",
      "===== RAM INFO =====\n",
      "Total RAM     : 503.67 GB\n",
      "Available RAM : 440.03 GB\n",
      "Used RAM      : 63.64 GB\n",
      "RAM Usage     : 12.6 %\n",
      "\n",
      "===== DISK INFO =====\n",
      "Total Disk    : 1758.86 GB\n",
      "Used Disk     : 1584.05 GB\n",
      "Free Disk     : 85.39 GB\n",
      "\n",
      "===== GPU / CUDA INFO =====\n",
      "CUDA Available: True\n",
      "CUDA Version  : 12.8\n",
      "GPU Count     : 2\n",
      "\n",
      "[GPU 0]\n",
      "Name          : NVIDIA RTX A6000\n",
      "Total VRAM    : 47.43 GB\n",
      "Allocated VRAM: 14.94 GB\n",
      "Reserved VRAM : 15.24 GB\n",
      "\n",
      "[GPU 1]\n",
      "Name          : NVIDIA RTX A6000\n",
      "Total VRAM    : 47.43 GB\n",
      "Allocated VRAM: 0.00 GB\n",
      "Reserved VRAM : 0.00 GB\n",
      "\n",
      "===== NVIDIA-SMI (if available) =====\n",
      "Thu Jan 15 13:54:52 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 45%   71C    P0            241W /  300W |   47613MiB /  49140MiB |     97%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  |   00000000:21:00.0 Off |                  Off |\n",
      "| 42%   69C    P0            240W /  300W |   47348MiB /  49140MiB |     98%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               On  |   00000000:22:00.0 Off |                  Off |\n",
      "| 35%   63C    P0            237W /  300W |   47348MiB /  49140MiB |     98%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               On  |   00000000:41:00.0 Off |                  Off |\n",
      "| 42%   69C    P0            239W /  300W |   47348MiB /  49140MiB |     98%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000               On  |   00000000:81:00.0 Off |                  Off |\n",
      "| 30%   35C    P0             70W /  300W |   15936MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000               On  |   00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   29C    P8             18W /  300W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000               On  |   00000000:C1:00.0 Off |                  Off |\n",
      "| 30%   28C    P8             22W /  300W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000               On  |   00000000:C2:00.0 Off |                  Off |\n",
      "| 30%   31C    P8             28W /  300W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    769880      C   ...ngjh/Research/ECO/.venv/bin/python3        260MiB |\n",
      "|    0   N/A  N/A   2825228      C   VLLM::Worker_TP0                            47338MiB |\n",
      "|    1   N/A  N/A   2825229      C   VLLM::Worker_TP1                            47338MiB |\n",
      "|    2   N/A  N/A   2825230      C   VLLM::Worker_TP2                            47338MiB |\n",
      "|    3   N/A  N/A   2825231      C   VLLM::Worker_TP3                            47338MiB |\n",
      "|    4   N/A  N/A    386780      C   ...pection-playground/.venv/bin/python      15926MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "\n",
    "print(\"===== BASIC SYSTEM INFO =====\")\n",
    "print(f\"OS            : {platform.system()} {platform.release()}\")\n",
    "print(f\"Python        : {platform.python_version()}\")\n",
    "print(f\"CPU           : {platform.processor()}\")\n",
    "print(f\"CPU Cores     : {psutil.cpu_count(logical=True)}\")\n",
    "\n",
    "print(\"\\n===== RAM INFO =====\")\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"Total RAM     : {ram.total / (1024**3):.2f} GB\")\n",
    "print(f\"Available RAM : {ram.available / (1024**3):.2f} GB\")\n",
    "print(f\"Used RAM      : {ram.used / (1024**3):.2f} GB\")\n",
    "print(f\"RAM Usage     : {ram.percent} %\")\n",
    "\n",
    "print(\"\\n===== DISK INFO =====\")\n",
    "disk = shutil.disk_usage(\"/\")\n",
    "print(f\"Total Disk    : {disk.total / (1024**3):.2f} GB\")\n",
    "print(f\"Used Disk     : {disk.used / (1024**3):.2f} GB\")\n",
    "print(f\"Free Disk     : {disk.free / (1024**3):.2f} GB\")\n",
    "\n",
    "print(\"\\n===== GPU / CUDA INFO =====\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version  : {torch.version.cuda}\")\n",
    "    print(f\"GPU Count     : {torch.cuda.device_count()}\")\n",
    "\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        total_vram = props.total_memory / (1024**3)\n",
    "        allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "\n",
    "        print(f\"\\n[GPU {i}]\")\n",
    "        print(f\"Name          : {props.name}\")\n",
    "        print(f\"Total VRAM    : {total_vram:.2f} GB\")\n",
    "        print(f\"Allocated VRAM: {allocated:.2f} GB\")\n",
    "        print(f\"Reserved VRAM : {reserved:.2f} GB\")\n",
    "else:\n",
    "    print(\"GPU not detected\")\n",
    "\n",
    "print(\"\\n===== NVIDIA-SMI (if available) =====\")\n",
    "try:\n",
    "    smi = subprocess.check_output([\"nvidia-smi\"], encoding=\"utf-8\")\n",
    "    print(smi)\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local modules loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "# 현재 노트의 디렉토리를 path에 추가\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "try:\n",
    "    from modeling_llada import LLaDAModelLM\n",
    "    from configuration_llada import LLaDAConfig\n",
    "    from decoding import add_gumbel_noise, get_num_transfer_tokens\n",
    "    print(\"Local modules loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Critical Error: {e}\")\n",
    "\n",
    "# 시각화 설정\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import benchmark_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model not found. Using HuggingFace Hub: GSAI-ML/LLaDA-8B-Base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc92148044e547a4bfea17789cc1de48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# 설정: 로컬 경로가 있으면 우선 사용, 없으면 HF Hub 사용\n",
    "LOCAL_MODEL_PATH = \"../Grok-1-LLaDA-8B\"\n",
    "HF_MODEL_ID = \"GSAI-ML/LLaDA-8B-Base\"\n",
    "\n",
    "model_path = HF_MODEL_ID\n",
    "if os.path.exists(LOCAL_MODEL_PATH):\n",
    "    model_path = LOCAL_MODEL_PATH\n",
    "    print(f\"Found local model at {model_path}\")\n",
    "else:\n",
    "    print(f\"Local model not found. Using HuggingFace Hub: {model_path}\")\n",
    "\n",
    "try:\n",
    "    config = LLaDAConfig.from_pretrained(model_path)\n",
    "    # GPU 메모리에 맞게 torch_dtype 설정 (기본: auto)\n",
    "    model = LLaDAModelLM.from_pretrained(model_path, config=config, torch_dtype=\"auto\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Model load failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: Standard Diffusion Sampling\n",
    "\n",
    "가장 기본적인 LLaDA 샘플링 방식입니다.\n",
    "*   **특징**: `steps`에 따라 정해진 수의 토큰을 Confidence 순으로 확정(Transfer)합니다. 한 번 확정된 토큰은 다시 바뀌지 않습니다 (No Remasking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def baseline_sampling(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    steps=64,\n",
    "    gen_length=64,\n",
    "    block_length=64,\n",
    "    temperature=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Baseline: Standard Iterative Decoding (No Remasking)\n",
    "    \"\"\"\n",
    "    # Init\n",
    "    mask_id = 126336\n",
    "    if prompt_text:\n",
    "        prompt_tokens = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(\n",
    "            model.device\n",
    "        )\n",
    "    else:\n",
    "        prompt_tokens = torch.tensor([[]], dtype=torch.long, device=model.device)\n",
    "\n",
    "    B, L_prompt = prompt_tokens.shape\n",
    "    x = torch.full(\n",
    "        (B, L_prompt + gen_length), mask_id, dtype=torch.long, device=model.device\n",
    "    )\n",
    "    x[:, :L_prompt] = prompt_tokens\n",
    "\n",
    "    num_blocks = gen_length // block_length\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    history = []\n",
    "    start_time = time.time()\n",
    "    nfe = 0  # Number of Function Evaluations (Forward passes)\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        block_start = L_prompt + num_block * block_length\n",
    "        block_end = L_prompt + (num_block + 1) * block_length\n",
    "\n",
    "        block_mask_index = x[:, block_start:block_end] == mask_id\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "\n",
    "        for i in range(steps):\n",
    "            logits = model(x).logits\n",
    "            nfe += 1\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "\n",
    "            p = F.softmax(logits.to(torch.float64), dim=-1)\n",
    "            x0_p = torch.squeeze(torch.gather(p, dim=-1, index=x0.unsqueeze(-1)), -1)\n",
    "\n",
    "            mask_index = x == mask_id\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            if i < num_transfer_tokens.shape[1]:\n",
    "                k = num_transfer_tokens[0, i].item()\n",
    "            else:\n",
    "                k = 0\n",
    "\n",
    "            # Standard Transfer: Top-k confidence tokens are unmasked\n",
    "            top_values, top_indices = torch.topk(confidence[0], k=k)\n",
    "            transfer_mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "            transfer_mask[0, top_indices] = True\n",
    "            x[transfer_mask] = x0[transfer_mask]\n",
    "\n",
    "            # Logging\n",
    "            history.append(\n",
    "                {\n",
    "                    \"step\": i,\n",
    "                    \"block\": num_block,\n",
    "                    \"nfe\": nfe,\n",
    "                    \"avg_confidence\": x0_p.mean().item(),\n",
    "                    \"text\": tokenizer.decode(x[0], skip_special_tokens=True),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return x, history, {\"time\": total_time, \"nfe\": nfe}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Deep Inspection Tools\n",
    "\n",
    "모델의 내부 동작을 상세히 분석하기 위한 도구들입니다.\n",
    "- `inspect_sampling`: 각 스텝별 토큰의 변화와 확신도(Confidence)를 모두 기록합니다.\n",
    "- `visualize_text_evolution`: 생성 과정을 HTML로 시각화하여 보여줍니다.\n",
    "- `plot_confidence_heatmap`: 스텝별 토큰의 확신도 변화를 히트맵으로 그립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inspect_sampling(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    steps=64,\n",
    "    gen_length=64,\n",
    "    block_length=64,\n",
    "    temperature=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inspection: Detailed logging of the sampling process.\n",
    "    Returns detailed history including token ids, predicted ids, and confidence at each step.\n",
    "    \"\"\"\n",
    "    mask_id = 126336\n",
    "    if prompt_text:\n",
    "        prompt_tokens = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(\n",
    "            model.device\n",
    "        )\n",
    "    else:\n",
    "        prompt_tokens = torch.tensor([[]], dtype=torch.long, device=model.device)\n",
    "\n",
    "    B, L_prompt = prompt_tokens.shape\n",
    "    x = torch.full(\n",
    "        (B, L_prompt + gen_length), mask_id, dtype=torch.long, device=model.device\n",
    "    )\n",
    "    x[:, :L_prompt] = prompt_tokens\n",
    "\n",
    "    num_blocks = max(1, gen_length // block_length)\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    detailed_history = []\n",
    "    nfe = 0\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        block_start = L_prompt + num_block * block_length\n",
    "        block_end = L_prompt + (num_block + 1) * block_length\n",
    "\n",
    "        block_mask_index = x[:, block_start:block_end] == mask_id\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
    "\n",
    "        for i in range(steps):\n",
    "            logits = model(x).logits\n",
    "            nfe += 1\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "\n",
    "            p = F.softmax(logits.to(torch.float64), dim=-1)\n",
    "            x0_p = torch.squeeze(torch.gather(p, dim=-1, index=x0.unsqueeze(-1)), -1)\n",
    "\n",
    "            mask_index = x == mask_id\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            # Capture State BEFORE update\n",
    "            step_info = {\n",
    "                \"step\": i + num_block * steps,\n",
    "                \"nfe\": nfe,\n",
    "                \"x_curr\": x.clone().cpu(),\n",
    "                \"x0_pred\": x0.clone().cpu(),\n",
    "                \"confidence\": x0_p.clone().cpu(),\n",
    "                \"mask_mask\": mask_index.clone().cpu(),\n",
    "            }\n",
    "            detailed_history.append(step_info)\n",
    "\n",
    "            if i < num_transfer_tokens.shape[1]:\n",
    "                k = num_transfer_tokens[0, i].item()\n",
    "            else:\n",
    "                k = 0\n",
    "\n",
    "            top_values, top_indices = torch.topk(confidence[0], k=k)\n",
    "            transfer_mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "            transfer_mask[0, top_indices] = True\n",
    "            x[transfer_mask] = x0[transfer_mask]\n",
    "\n",
    "    return x, detailed_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import html\n",
    "\n",
    "\n",
    "def visualize_text_evolution(history, tokenizer, show_steps_stride=4):\n",
    "    \"\"\"\n",
    "    Visualizes the evolution of text generation.\n",
    "    Black: Prompt / Fixed tokens\n",
    "    Blue (Bold): Newly fixed tokens in this step\n",
    "    Gray (Italic): Masked tokens (prediction)\n",
    "    \"\"\"\n",
    "    mask_id = 126336\n",
    "    html_out = \"<div style='font-family: monospace; line-height: 1.5;'>\"\n",
    "\n",
    "    prev_x = None\n",
    "\n",
    "    for step_data in history:\n",
    "        step = step_data[\"step\"]\n",
    "        if step % show_steps_stride != 0 and step != 0 and step != len(history) - 1:\n",
    "            continue\n",
    "\n",
    "        x_curr = step_data[\"x_curr\"][0]\n",
    "        x0_pred = step_data[\"x0_pred\"][0]\n",
    "        mask_mask = step_data[\"mask_mask\"][0]\n",
    "\n",
    "        # Determine newly fixed tokens compared to previous step\n",
    "        if prev_x is None:\n",
    "            newly_fixed = torch.zeros_like(x_curr, dtype=torch.bool)\n",
    "        else:\n",
    "            # Fixed now (not mask) AND was mask before\n",
    "            newly_fixed = (x_curr != mask_id) & (prev_x == mask_id)\n",
    "\n",
    "        step_html = f\"<div style='margin-bottom: 5px;'><span style='color: #888;'>Step {step}:</span> \"\n",
    "\n",
    "        for idx, token_id in enumerate(x_curr):\n",
    "            is_mask = mask_mask[idx].item()\n",
    "            is_new = newly_fixed[idx].item() if prev_x is not None else False\n",
    "\n",
    "            if is_mask:\n",
    "                # Show prediction in gray\n",
    "                text = tokenizer.decode([x0_pred[idx]], skip_special_tokens=True)\n",
    "                # Handle spaces for visualization\n",
    "                text = html.escape(text).replace(\" \", \"&nbsp;\")\n",
    "                if not text:\n",
    "                    text = \"□\"  # placeholder for empty\n",
    "                step_html += (\n",
    "                    f\"<span style='color: #aaa; font-style: italic;'>{text}</span>\"\n",
    "                )\n",
    "            elif is_new:\n",
    "                # Show newly fixed in blue bold\n",
    "                text = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "                text = html.escape(text).replace(\" \", \"&nbsp;\")\n",
    "                step_html += (\n",
    "                    f\"<span style='color: #007bff; font-weight: bold;'>{text}</span>\"\n",
    "                )\n",
    "            else:\n",
    "                # Fixed info\n",
    "                text = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "                text = html.escape(text).replace(\" \", \"&nbsp;\")\n",
    "                step_html += f\"<span style='color: black;'>{text}</span>\"\n",
    "\n",
    "        step_html += \"</div>\"\n",
    "        html_out += step_html\n",
    "        prev_x = x_curr\n",
    "\n",
    "    html_out += \"</div>\"\n",
    "    display(HTML(html_out))\n",
    "\n",
    "\n",
    "def plot_confidence_heatmap(history, gen_length_offset=0):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of token confidence over steps.\n",
    "    \"\"\"\n",
    "    # Aggregate confidence scores into a matrix (Steps x Sequence Length)\n",
    "    # Note: history items might have different lengths if we supported dynamic resizing, but here logic is fixed len\n",
    "\n",
    "    conf_matrix = []\n",
    "    for step_data in history:\n",
    "        conf_matrix.append(step_data[\"confidence\"][0].numpy())\n",
    "\n",
    "    conf_matrix = np.array(conf_matrix)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        conf_matrix,\n",
    "        cmap=\"viridis\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cbar_kws={\"label\": \"Confidence\"},\n",
    "    )\n",
    "    plt.title(\"Token Confidence Evolution (Steps x Sequence)\")\n",
    "    plt.xlabel(\"Token Position\")\n",
    "    plt.ylabel(\"Generation Step\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Inspection 실행\n",
    "prompt = \"The capital of France is\"\n",
    "print(f\"Inspecting sampling for prompt: '{prompt}'\")\n",
    "\n",
    "res, details = inspect_sampling(model, tokenizer, prompt, steps=64, gen_length=20)\n",
    "\n",
    "print(\"\\n[Visualization: Text Evolution]\")\n",
    "visualize_text_evolution(details, tokenizer, show_steps_stride=8)\n",
    "\n",
    "print(\"\\n[Visualization: Confidence Heatmap]\")\n",
    "plot_confidence_heatmap(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental: Adaptive & Risk-aware Sampling\n",
    "\n",
    "이곳에는 **Training-free** 기반의 새로운 샘플링기법이 구현되어 있습니다.\n",
    "- **Adaptive Transfer**: 불확실성에 따라 확정 속도 조절\n",
    "- **Risk-aware Selection**: Margin을 고려한 안전한 토큰 선택\n",
    "- **Budgeted Remasking**: 제한된 예산 내에서 오류 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def experimental_sampling(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    steps=64,\n",
    "    gen_length=64,\n",
    "    block_length=64,\n",
    "    temperature=0.0,\n",
    "    alpha_margin=0.1,\n",
    "    remask_budget=0.05,\n",
    "    remask_threshold=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Experimental: Adaptive, Risk-aware, Budgeted Remasking with History Tracking\n",
    "    \"\"\"\n",
    "    mask_id = 126336\n",
    "    if prompt_text:\n",
    "        prompt_tokens = tokenizer.encode(prompt_text, return_tensors='pt').to(model.device)\n",
    "    else:\n",
    "        prompt_tokens = torch.tensor([[]], dtype=torch.long, device=model.device)\n",
    "\n",
    "    B, L_prompt = prompt_tokens.shape\n",
    "    x = torch.full((B, L_prompt + gen_length), mask_id, dtype=torch.long, device=model.device)\n",
    "    x[:, :L_prompt] = prompt_tokens\n",
    "    \n",
    "    num_blocks = max(1, gen_length // block_length)\n",
    "    steps = steps // num_blocks\n",
    "    \n",
    "    # Logging Init\n",
    "    step_logs = []\n",
    "    # Track x0 prediction history for Stability Score\n",
    "    # Shape: [Steps, Gen_Length]\n",
    "    x0_history = []\n",
    "    \n",
    "    # Token Log: Key=Index, Val={fix_step, fix_token, fix_conf, remask_events: [(step, prev_token, new_conf)]}\n",
    "    token_logs = {i: {'fix_step': -1, 'fix_token': -1, 'fix_conf': 0.0, 'remask_events': []} for i in range(L_prompt + gen_length)}\n",
    "\n",
    "    start_time = time.time()\n",
    "    nfe = 0\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        block_start = L_prompt + num_block * block_length\n",
    "        block_end = L_prompt + (num_block + 1) * block_length\n",
    "        \n",
    "        for i in range(steps):\n",
    "            logits = model(x).logits\n",
    "            nfe += 1\n",
    "            \n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "            \n",
    "            # Capture x0 for stability analysis\n",
    "            x0_history.append(x0[0].cpu().clone())\n",
    "\n",
    "            probs = F.softmax(logits.to(torch.float64), dim=-1)\n",
    "            x0_p = torch.squeeze(torch.gather(probs, dim=-1, index=x0.unsqueeze(-1)), -1)\n",
    "            \n",
    "            top2_vals, _ = torch.topk(probs, k=2, dim=-1)\n",
    "            margin = top2_vals[:, :, 0] - top2_vals[:, :, 1]\n",
    "            \n",
    "            mask_index = (x == mask_id)\n",
    "            \n",
    "            # Risk Score\n",
    "            risk_score = x0_p + alpha_margin * margin\n",
    "            transfer_candidate_score = torch.where(mask_index, risk_score, -np.inf)\n",
    "            \n",
    "            # Adaptive Budget\n",
    "            current_masks = mask_index.sum().item()\n",
    "            if current_masks == 0: break\n",
    "            \n",
    "            base_k = max(1, current_masks // (steps - i + 1))\n",
    "            candidate_confs = x0_p[mask_index]\n",
    "            k_t = base_k\n",
    "            if len(candidate_confs) > 0:\n",
    "                mean_conf = candidate_confs.mean().item()\n",
    "                adaptive_factor = max(0.5, mean_conf)\n",
    "                k_t = max(1, int(base_k * adaptive_factor))\n",
    "\n",
    "            # Transfer\n",
    "            top_scores, top_indices = torch.topk(transfer_candidate_score[0], k=k_t)\n",
    "            transfer_mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "            transfer_mask[0, top_indices] = True\n",
    "            x[transfer_mask] = x0[transfer_mask]\n",
    "            \n",
    "            # Log Fixes\n",
    "            for idx in top_indices.cpu().numpy():\n",
    "                token_logs[idx]['fix_step'] = i + num_block * steps\n",
    "                token_logs[idx]['fix_token'] = x0[0, idx].item()\n",
    "                token_logs[idx]['fix_conf'] = x0_p[0, idx].item()\n",
    "\n",
    "            # Remasking\n",
    "            generated_mask = (x != mask_id)\n",
    "            generated_mask[:, :L_prompt] = False \n",
    "            \n",
    "            current_token_prob = torch.squeeze(torch.gather(probs, dim=-1, index=x.unsqueeze(-1)), -1)\n",
    "            remask_candidates_mask = generated_mask & (current_token_prob < remask_threshold)\n",
    "            \n",
    "            r_t = 0\n",
    "            if remask_candidates_mask.any():\n",
    "                max_remask = int(gen_length * remask_budget)\n",
    "                num_candidates = remask_candidates_mask.sum().item()\n",
    "                r_t = min(num_candidates, max_remask, k_t)\n",
    "                \n",
    "                if r_t > 0:\n",
    "                    cand_scores = torch.where(remask_candidates_mask, current_token_prob, np.inf)\n",
    "                    _, remask_indices = torch.topk(cand_scores[0], k=r_t, largest=False)\n",
    "                    \n",
    "                    # Log Before Masking\n",
    "                    for idx in remask_indices.cpu().numpy():\n",
    "                        token_logs[idx]['remask_events'].append({\n",
    "                            'step': i + num_block*steps,\n",
    "                            'old_token': x[0, idx].item(),\n",
    "                            'old_conf': current_token_prob[0, idx].item()\n",
    "                        })\n",
    "                    \n",
    "                    x[0, remask_indices] = mask_id\n",
    "\n",
    "            step_logs.append({\n",
    "                'step': i + num_block * steps,\n",
    "                'k_t': k_t,\n",
    "                'r_t': r_t,\n",
    "                'mask_ratio': mask_index.float().mean().item(),\n",
    "            })\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return x, {'step_logs': step_logs, 'token_logs': token_logs, 'x0_history': x0_history, 'metrics': {'time': total_time, 'nfe': nfe}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison Runner\n",
    "\n",
    "Baseline과 Experimental 모델을 동시에 실행하고 성능과 행동 패턴을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    Calculates Perplexity (PPL) of the generated text using the model itself.\n",
    "    Note: For diffusion models, this is a 'Pseudo-PPL' or 'Reconstruction PPL'.\n",
    "    We feed the text to the model and measure the CrossEntropyLoss of predicting it.\n",
    "    \"\"\"\n",
    "    if not text.strip(): return 0.0\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "    input_ids = inputs.input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Pass input_ids as both input and target logic\n",
    "        # LLaDAModelLM.forward doesn't compute loss, so we compute it manually from logits.\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits # [1, SeqLen, VocabSize]\n",
    "        \n",
    "        # Shift logits and labels for autoregressive-style scoring?\n",
    "        # No, LLaDA is non-autoregressive (BERT-like) in structure but often trained with diffusion.\n",
    "        # However, for 'how well does the model know this text', we can just check probability of each token given context.\n",
    "        # But wait, LLaDA 'forward' without masks sees the whole answer. Trivial?\n",
    "        # If we want a meaningful PPL, we technically should mask tokens one by one (expensive).\n",
    "        # OR, we assume the model output distribution should assign high prob to the token present.\n",
    "        \n",
    "        # Let's use simple CE Loss: P(Token_i | All_Tokens)\n",
    "        # Ideally, we should mask Token_i, but that requires N passes.\n",
    "        # For efficiency in this playground, we use the unmasked forward pass (Optimistic PPL).\n",
    "        # It will likely be very low, but comparable between Baseline and Custom.\n",
    "        \n",
    "        shift_logits = logits.view(-1, logits.size(-1))\n",
    "        shift_labels = input_ids.view(-1)\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "        \n",
    "        ppl = torch.exp(loss)\n",
    "        return ppl.item()\n",
    "\n",
    "def calculate_diversity(text):\n",
    "    \"\"\"\n",
    "    Calculates Distinct-1 and Distinct-2 scores.\n",
    "    \"\"\"\n",
    "    if not text: return 0.0, 0.0\n",
    "    tokens = text.split()\n",
    "    if not tokens: return 0.0, 0.0\n",
    "    \n",
    "    # Dist-1\n",
    "    unique_1 = len(set(tokens))\n",
    "    total_1 = len(tokens)\n",
    "    dist_1 = unique_1 / total_1 if total_1 > 0 else 0.0\n",
    "    \n",
    "    # Dist-2\n",
    "    bi_grams = list(zip(tokens[:-1], tokens[1:]))\n",
    "    if not bi_grams: return dist_1, 0.0\n",
    "    \n",
    "    unique_2 = len(set(bi_grams))\n",
    "    total_2 = len(bi_grams)\n",
    "    dist_2 = unique_2 / total_2 if total_2 > 0 else 0.0\n",
    "    \n",
    "    return dist_1, dist_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionMetrics:\n",
    "    @staticmethod\n",
    "    def compute_stability(x0_history):\n",
    "        \"\"\"\n",
    "        Stability Score: % of time the predicted token stays the same as previous step.\n",
    "        \"\"\"\n",
    "        if not x0_history: return 0.0\n",
    "        # Stack history: [Steps, SeqLen]\n",
    "        # Ensure all tensors are on same device\n",
    "        device = x0_history[0].device\n",
    "        hist = torch.stack(x0_history).to(device)\n",
    "        if hist.shape[0] < 2: return 1.0\n",
    "        \n",
    "        # Compare t with t-1\n",
    "        changes = (hist[1:] != hist[:-1]).float()\n",
    "        # Stability = 1 - change_rate\n",
    "        mean_change = changes.mean().item()\n",
    "        return 1.0 - mean_change\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_correction_efficacy(token_logs, final_tokens):\n",
    "        \"\"\"\n",
    "        Did remasking lead to a different (and hopefully better) token?\n",
    "        \"\"\"\n",
    "        remask_count = 0\n",
    "        changed_count = 0\n",
    "        \n",
    "        for idx, log in token_logs.items():\n",
    "            events = log['remask_events']\n",
    "            if not events: continue\n",
    "            \n",
    "            remask_count += len(events)\n",
    "            first_fix = log['fix_token']\n",
    "            final_tok = final_tokens[idx].item() if idx < len(final_tokens) else -1\n",
    "            \n",
    "            if first_fix != -1 and final_tok != first_fix:\n",
    "                changed_count += 1\n",
    "        \n",
    "        efficacy = changed_count / remask_count if remask_count > 0 else 0.0\n",
    "        return efficacy, remask_count\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_survival_rate(token_logs, final_tokens):\n",
    "        \"\"\"\n",
    "        Survival Rate: % of first-fixed tokens that survived to the end.\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        survived = 0\n",
    "        \n",
    "        for idx, log in token_logs.items():\n",
    "            first_fix = log['fix_token']\n",
    "            if first_fix == -1: continue # Never fixed (prompt?)\n",
    "            \n",
    "            total += 1\n",
    "            final_tok = final_tokens[idx].item() if idx < len(final_tokens) else -2\n",
    "            \n",
    "            if final_tok == first_fix:\n",
    "                survived += 1\n",
    "        \n",
    "        return survived / total if total > 0 else 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [ICML UPGRADE] Academic Benchmark Implementation\n",
    "# Replaces 'MINI_BENCHMARK' with GSM8K and MMLU\n",
    "\n",
    "def run_academic_benchmark(thresholds=[0.3, 0.4, 0.5, 0.6], samples=50):\n",
    "    print(f\"Loading Academic Benchmarks (N={samples} per task)...\")\n",
    "    \n",
    "    # Load via benchmark_utils\n",
    "    gsm8k_data = benchmark_utils.load_gsm8k(n_samples=samples)\n",
    "    mmlu_data = benchmark_utils.load_mmlu_logic(n_samples=samples)\n",
    "    \n",
    "    # Combine\n",
    "    full_dataset = gsm8k_data + mmlu_data\n",
    "    print(f\"Loaded {len(full_dataset)} total samples.\")\n",
    "    \n",
    "    results = []\n",
    "    total_runs = len(full_dataset) * len(thresholds)\n",
    "    current_run = 0\n",
    "    \n",
    "    steps = 64 # Fixed for consistency\n",
    "    \n",
    "    for item in full_dataset:\n",
    "        category = item['category']\n",
    "        prompt = item['question']\n",
    "        ground_truth = item['ground_truth']\n",
    "        \n",
    "        # 1. Baseline (Standard Sampling)\n",
    "        # Run once per prompt\n",
    "        res_base, hist_base = inspect_sampling(model, tokenizer, prompt, steps=steps)\n",
    "        text_base = tokenizer.decode(res_base[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Metrics Base\n",
    "        ppl_base = calculate_perplexity(model, tokenizer, text_base)\n",
    "        correct_base = benchmark_utils.check_correctness(text_base, ground_truth, category)\n",
    "        stab_base = DiffusionMetrics.compute_stability([h['x0_pred'][0] for h in hist_base])\n",
    "        \n",
    "        for th in thresholds:\n",
    "            current_run += 1\n",
    "            if current_run % 10 == 0:\n",
    "                print(f\"Progress: [{current_run}/{total_runs}]\")\n",
    "                \n",
    "            # 2. Experimental (Proposed Method)\n",
    "            res_exp, logs_exp = experimental_sampling(model, tokenizer, prompt, steps=steps, remask_threshold=th)\n",
    "            text_exp = tokenizer.decode(res_exp[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Metrics Exp\n",
    "            ppl_exp = calculate_perplexity(model, tokenizer, text_exp)\n",
    "            correct_exp = benchmark_utils.check_correctness(text_exp, ground_truth, category)\n",
    "            stab_exp = DiffusionMetrics.compute_stability(logs_exp['x0_history'])\n",
    "            eff_exp, _ = DiffusionMetrics.compute_correction_efficacy(logs_exp['token_logs'], res_exp[0])\n",
    "            surv_exp = DiffusionMetrics.compute_survival_rate(logs_exp['token_logs'], res_exp[0])\n",
    "            \n",
    "            results.append({\n",
    "                \"Category\": category,\n",
    "                \"Prompt\": prompt,\n",
    "                \"GroundTruth\": ground_truth,\n",
    "                \"Threshold\": th,\n",
    "                # Accuracy (Boolean 1/0 for averaging)\n",
    "                \"Acc_Base\": 1.0 if correct_base else 0.0,\n",
    "                \"Acc_Exp\": 1.0 if correct_exp else 0.0,\n",
    "                \"Acc_Delta\": (1.0 if correct_exp else 0.0) - (1.0 if correct_base else 0.0),\n",
    "                # PPL\n",
    "                \"PPL_Base\": ppl_base,\n",
    "                \"PPL_Exp\": ppl_exp,\n",
    "                \"PPL_Delta\": ppl_exp - ppl_base,\n",
    "                # Stability\n",
    "                \"Stability_Delta\": stab_exp - stab_base,\n",
    "                \"Survival\": surv_exp,\n",
    "                \"Correction_Eff\": eff_exp\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_icml_results(df):\n",
    "    print(\"\\n===== ICML Benchmark Results =====\")\n",
    "    \n",
    "    # 1. Main Table: Accuracy & PPL per Threshold\n",
    "    # Ensure numeric columns\n",
    "    numeric_cols = [\"Acc_Base\", \"Acc_Exp\", \"Acc_Delta\", \"PPL_Delta\", \"Stability_Delta\"]\n",
    "    summary = df.groupby([\"Category\", \"Threshold\"])[numeric_cols].mean()\n",
    "    display(summary)\n",
    "    \n",
    "    # 2. Visualization\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(data=df, x=\"Threshold\", y=\"Acc_Exp\", hue=\"Category\")\n",
    "    plt.axhline(df[\"Acc_Base\"].mean(), color='red', linestyle='--', label=\"Baseline Avg\")\n",
    "    plt.title(\"Accuracy vs Threshold (Higher is Better)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # PPL Delta Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(data=df, x=\"Threshold\", y=\"PPL_Delta\", hue=\"Category\", marker=\"o\")\n",
    "    plt.axhline(0, color='black', linestyle='--')\n",
    "    plt.title(\"Perplexity Delta (Lower is Better)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Statistical Highlight\n",
    "    print(\"\\n[Key Findings]\")\n",
    "    # Mean across all categories for each threshold\n",
    "    global_stats = df.groupby(\"Threshold\")[\"Acc_Exp\"].mean()\n",
    "    best_th = global_stats.idxmax()\n",
    "    best_acc = global_stats.max()\n",
    "    base_acc = df[\"Acc_Base\"].mean()\n",
    "    \n",
    "    print(f\"Best Threshold: {best_th}\")\n",
    "    print(f\"Optimal Accuracy: {best_acc:.2%}\")\n",
    "    print(f\"Baseline Accuracy: {base_acc:.2%}\")\n",
    "    print(f\"Improvement: {best_acc - base_acc:+.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run Academic Benchmark\n",
    "if \"model\" in locals():\n",
    "    # Running small sample for quick verification in notebook (N=10)\n",
    "    # For full paper results, increase samples to 50 or 100\n",
    "    df_results = run_academic_benchmark(thresholds=[0.3, 0.4, 0.5, 0.6], samples=10)\n",
    "    analyze_icml_results(df_results)\n",
    "else:\n",
    "    print(\"Please load the model first (Step 1).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
