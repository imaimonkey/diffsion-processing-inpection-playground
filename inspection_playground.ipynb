{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLaDA Diffusion Process Inspection & Experimentation\n",
                "\n",
                "이 노트북은 LLaDA 모델의 디퓨전 기반 텍스트 생성 과정을 단계별로 분석하고, **새로운 샘플링 알고리즘을 실험하여 베이스라인과 비교**하기 위해 작성되었습니다.\n",
                "\n",
                "## 주요 기능\n",
                "1. **Inspection**: 디퓨전 과정의 투명한 시각화 (Step-by-step Logits, Confidence).\n",
                "2. **Experimentation**: 커스텀 샘플링 알고리즘 구현 및 적용.\n",
                "3. **Comparison**: Baseline vs Custom 알고리즘의 성능(NFE, 속도, 품질) 정량 비교."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import time\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
                "\n",
                "# 현재 노트의 디렉토리를 path에 추가\n",
                "current_dir = os.getcwd()\n",
                "if current_dir not in sys.path:\n",
                "    sys.path.append(current_dir)\n",
                "\n",
                "try:\n",
                "    from modeling_llada import LLaDAModel\n",
                "    from configuration_llada import LLaDAConfig\n",
                "    from decoding import add_gumbel_noise, get_num_transfer_tokens\n",
                "    print(\"Local modules loaded successfully.\")\n",
                "except ImportError as e:\n",
                "    print(f\"Critical Error: {e}\")\n",
                "\n",
                "# 시각화 설정\n",
                "%matplotlib inline\n",
                "plt.rcParams['figure.figsize'] = [12, 6]\n",
                "sns.set_theme(style=\"whitegrid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 모델 로드"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 설정: 로컬 경로가 있으면 우선 사용, 없으면 HF Hub 사용\n",
                "LOCAL_MODEL_PATH = \"../Grok-1-LLaDA-8B\"\n",
                "HF_MODEL_ID = \"GSAI-ML/LLaDA-8B-Base\"\n",
                "\n",
                "model_path = HF_MODEL_ID\n",
                "if os.path.exists(LOCAL_MODEL_PATH):\n",
                "    model_path = LOCAL_MODEL_PATH\n",
                "    print(f\"Found local model at {model_path}\")\n",
                "else:\n",
                "    print(f\"Local model not found. Using HuggingFace Hub: {model_path}\")\n",
                "\n",
                "try:\n",
                "    config = LLaDAConfig.from_pretrained(model_path)\n",
                "    # GPU 메모리에 맞게 torch_dtype 설정 (기본: auto)\n",
                "    model = LLaDAModel.from_pretrained(model_path, config=config, torch_dtype=\"auto\")\n",
                "    \n",
                "    if torch.cuda.is_available():\n",
                "        model.cuda()\n",
                "    model.eval()\n",
                "    \n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "    print(\"Model loaded successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"Model load failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Baseline: Standard Diffusion Sampling\n",
                "\n",
                "가장 기본적인 LLaDA 샘플링 방식입니다.\n",
                "*   **특징**: `steps`에 따라 정해진 수의 토큰을 Confidence 순으로 확정(Transfer)합니다. 한 번 확정된 토큰은 다시 바뀌지 않습니다 (No Remasking)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.no_grad()\n",
                "def baseline_sampling(model, tokenizer, prompt_text, steps=64, gen_length=64, block_length=64, temperature=0.0):\n",
                "    \"\"\"\n",
                "    Baseline: Standard Iterative Decoding (No Remasking)\n",
                "    \"\"\"\n",
                "    # Init\n",
                "    mask_id = 126336\n",
                "    if prompt_text:\n",
                "        prompt_tokens = tokenizer.encode(prompt_text, return_tensors='pt').to(model.device)\n",
                "    else:\n",
                "        prompt_tokens = torch.tensor([[]], dtype=torch.long, device=model.device)\n",
                "\n",
                "    B, L_prompt = prompt_tokens.shape\n",
                "    x = torch.full((B, L_prompt + gen_length), mask_id, dtype=torch.long, device=model.device)\n",
                "    x[:, :L_prompt] = prompt_tokens\n",
                "    \n",
                "    num_blocks = gen_length // block_length\n",
                "    steps = steps // num_blocks\n",
                "    \n",
                "    history = []\n",
                "    start_time = time.time()\n",
                "    nfe = 0 # Number of Function Evaluations (Forward passes)\n",
                "\n",
                "    for num_block in range(num_blocks):\n",
                "        block_start = L_prompt + num_block * block_length\n",
                "        block_end = L_prompt + (num_block + 1) * block_length\n",
                "        \n",
                "        block_mask_index = (x[:, block_start:block_end] == mask_id)\n",
                "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\n",
                "\n",
                "        for i in range(steps):\n",
                "            logits = model(x).logits\n",
                "            nfe += 1\n",
                "            \n",
                "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
                "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
                "            \n",
                "            p = F.softmax(logits.to(torch.float64), dim=-1)\n",
                "            x0_p = torch.squeeze(torch.gather(p, dim=-1, index=x0.unsqueeze(-1)), -1)\n",
                "            \n",
                "            mask_index = (x == mask_id)\n",
                "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
                "            \n",
                "            if i < num_transfer_tokens.shape[1]:\n",
                "                k = num_transfer_tokens[0, i].item()\n",
                "            else: k = 0\n",
                "            \n",
                "            # Standard Transfer: Top-k confidence tokens are unmasked\n",
                "            top_values, top_indices = torch.topk(confidence[0], k=k)\n",
                "            transfer_mask = torch.zeros_like(x, dtype=torch.bool)\n",
                "            transfer_mask[0, top_indices] = True\n",
                "            x[transfer_mask] = x0[transfer_mask]\n",
                "            \n",
                "            # Logging\n",
                "            history.append({\n",
                "                'step': i, 'block': num_block, 'nfe': nfe,\n",
                "                'avg_confidence': x0_p.mean().item(),\n",
                "                'text': tokenizer.decode(x[0], skip_special_tokens=True)\n",
                "            })\n",
                "\n",
                "    total_time = time.time() - start_time\n",
                "    return x, history, {'time': total_time, 'nfe': nfe}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Custom: Your New Algorithm\n",
                "\n",
                "이곳에 **Backtracking(Remasking)** 등을 포함한 커스텀 알고리즘을 구현하세요.\n",
                "예시로, **Confidence Threshold**보다 낮은 토큰을 다시 마스킹하는 간단한 로직을 주석으로 포함했습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.no_grad()\n",
                "def custom_sampling(model, tokenizer, prompt_text, steps=64, gen_length=64, block_length=64, temperature=0.0):\n",
                "    \"\"\"\n",
                "    Custom: Experimental Sampling with Simple Remasking\n",
                "    \"\"\"\n",
                "    mask_id = 126336\n",
                "    if prompt_text:\n",
                "        prompt_tokens = tokenizer.encode(prompt_text, return_tensors='pt').to(model.device)\n",
                "    else:\n",
                "        prompt_tokens = torch.tensor([[]], dtype=torch.long, device=model.device)\n",
                "\n",
                "    B, L_prompt = prompt_tokens.shape\n",
                "    x = torch.full((B, L_prompt + gen_length), mask_id, dtype=torch.long, device=model.device)\n",
                "    x[:, :L_prompt] = prompt_tokens\n",
                "    \n",
                "    num_blocks = gen_length // block_length\n",
                "    steps = steps // num_blocks\n",
                "    \n",
                "    history = []\n",
                "    start_time = time.time()\n",
                "    nfe = 0\n",
                "\n",
                "    # --- [Custom Parameter] ---\n",
                "    remask_threshold = 0.4  # 이 값보다 confidence가 낮으면 다시 마스킹 시도\n",
                "    # --------------------------\n",
                "\n",
                "    for num_block in range(num_blocks):\n",
                "        block_start = L_prompt + num_block * block_length\n",
                "        block_end = L_prompt + (num_block + 1) * block_length\n",
                "        \n",
                "        # Custom Logic: 여기서는 스텝을 엄격하게 나누지 않고 while 루프나 유동적인 스텝을 사용할 수도 있습니다.\n",
                "        # 비교를 위해 baseline과 구조를 비슷하게 유지하되, remasking 로직을 추가해봅니다.\n",
                "        \n",
                "        for i in range(steps):\n",
                "            logits = model(x).logits\n",
                "            nfe += 1\n",
                "            \n",
                "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)\n",
                "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
                "            p = F.softmax(logits.to(torch.float64), dim=-1)\n",
                "            x0_p = torch.squeeze(torch.gather(p, dim=-1, index=x0.unsqueeze(-1)), -1)\n",
                "            \n",
                "            mask_index = (x == mask_id)\n",
                "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
                "            \n",
                "            # 1. Transfer (Generate)\n",
                "            # Baseline처럼 k개를 전송하되, 전체 steps 대신 현재 남은 마스크 비율 등을 고려할 수 있습니다.\n",
                "            current_masks = mask_index.sum().item()\n",
                "            if current_masks == 0: break\n",
                "            \n",
                "            k = max(1, current_masks // (steps - i + 1)) # Simple Linear Schedule\n",
                "            \n",
                "            top_values, top_indices = torch.topk(confidence[0], k=k)\n",
                "            transfer_mask = torch.zeros_like(x, dtype=torch.bool)\n",
                "            transfer_mask[0, top_indices] = True\n",
                "            x[transfer_mask] = x0[transfer_mask]\n",
                "            \n",
                "            # 2. Remasking (Correction)\n",
                "            # 이미 생성된 토큰들 중 (프롬프트 제외) confidence가 낮은 것을 다시 마스킹\n",
                "            generated_mask = (x != mask_id)\n",
                "            generated_mask[:, :L_prompt] = False # 프롬프트 보호\n",
                "            \n",
                "            # 현재 상태의 confidence 확인 (이미 생성된 토큰들의 확률)\n",
                "            # 주의: 정확한 구현을 위해선 x0_p가 현재 토큰의 확률이어야 함.\n",
                "            # 위에서 x0는 argmax이므로, 현재 x에 있는 토큰의 확률을 구하려면 gather를 x 기준으로 다시 해야 함.\n",
                "            current_token_p = torch.squeeze(torch.gather(p, dim=-1, index=x.unsqueeze(-1)), -1)\n",
                "            \n",
                "            low_conf_mask = (current_token_p < remask_threshold) & generated_mask\n",
                "            \n",
                "            if low_conf_mask.any():\n",
                "                # 너무 많이 지우면 무한루프 돌 수 있으므로 개수 제한 (예: k의 절반)\n",
                "                num_remask = min(low_conf_mask.sum().item(), max(1, k // 2))\n",
                "                # 가장 확신이 없는 것부터 지움\n",
                "                remask_candidates = torch.where(low_conf_mask, current_token_p, np.inf)\n",
                "                _, remask_indices = torch.topk(remask_candidates[0], k=num_remask, largest=False)\n",
                "                \n",
                "                x[0, remask_indices] = mask_id\n",
                "            \n",
                "            history.append({\n",
                "                'step': i, 'block': num_block, 'nfe': nfe,\n",
                "                'avg_confidence': current_token_p[generated_mask].mean().item() if generated_mask.any() else 0,\n",
                "                'text': tokenizer.decode(x[0], skip_special_tokens=True)\n",
                "            })\n",
                "\n",
                "    total_time = time.time() - start_time\n",
                "    return x, history, {'time': total_time, 'nfe': nfe}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Experiment Runner & Comparison\n",
                "\n",
                "동일한 프롬프트로 두 알고리즘을 실행하고 지표를 비교합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_experiment(prompt, steps=32):\n",
                "    print(f\"Experimenting with prompt: '{prompt}'\")\n",
                "    \n",
                "    # 1. Run Baseline\n",
                "    print(\"Running Baseline...\", end=\" \")\n",
                "    res_base, hist_base, stats_base = baseline_sampling(model, tokenizer, prompt, steps=steps)\n",
                "    print(f\"Done. (Time: {stats_base['time']:.2f}s, NFE: {stats_base['nfe']})\")\n",
                "    \n",
                "    # 2. Run Custom\n",
                "    print(\"Running Custom...\", end=\" \")\n",
                "    res_cust, hist_cust, stats_cust = custom_sampling(model, tokenizer, prompt, steps=steps)\n",
                "    print(f\"Done. (Time: {stats_cust['time']:.2f}s, NFE: {stats_cust['nfe']})\")\n",
                "    \n",
                "    # 3. Compare Results\n",
                "    text_base = tokenizer.decode(res_base[0], skip_special_tokens=True)\n",
                "    text_cust = tokenizer.decode(res_cust[0], skip_special_tokens=True)\n",
                "    \n",
                "    print(\"\\n--- [Outputs] ---\")\n",
                "    print(f\"[Baseline]: {text_base}\")\n",
                "    print(f\"[Custom]  : {text_cust}\")\n",
                "    \n",
                "    # 4. Visualization\n",
                "    # Compare Confidence Evolution\n",
                "    conf_base = [h['avg_confidence'] for h in hist_base]\n",
                "    conf_cust = [h['avg_confidence'] for h in hist_cust]\n",
                "    \n",
                "    plt.figure(figsize=(14, 5))\n",
                "    \n",
                "    # Plot 1: Confidence\n",
                "    plt.subplot(1, 2, 1)\n",
                "    plt.plot(conf_base, label=f'Baseline (NFE={stats_base[\"nfe\"]})', marker='o')\n",
                "    plt.plot(conf_cust, label=f'Custom (NFE={stats_cust[\"nfe\"]})', marker='x')\n",
                "    plt.title(\"Average Token Confidence over Steps\")\n",
                "    plt.xlabel(\"step\")\n",
                "    plt.ylabel(\"Confidence\")\n",
                "    plt.legend()\n",
                "    \n",
                "    # Plot 2: Cost (NFE) vs Time\n",
                "    plt.subplot(1, 2, 2)\n",
                "    metrics = ['Time (s)', 'NFE (Cost)']\n",
                "    x_pos = np.arange(len(metrics))\n",
                "    w = 0.35\n",
                "    \n",
                "    val_base = [stats_base['time'], stats_base['nfe']]\n",
                "    val_cust = [stats_cust['time'], stats_cust['nfe']]\n",
                "    \n",
                "    plt.bar(x_pos - w/2, val_base, w, label='Baseline')\n",
                "    plt.bar(x_pos + w/2, val_cust, w, label='Custom')\n",
                "    plt.xticks(x_pos, metrics)\n",
                "    plt.title(\"Efficiency Comparison\")\n",
                "    plt.legend()\n",
                "    \n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 실험 실행\n",
                "# 모델이 로드되어 있어야 합니다.\n",
                "if 'model' in locals():\n",
                "    run_experiment(\"Python function to merge sort:\", steps=20)\n",
                "else:\n",
                "    print(\"load model first\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}